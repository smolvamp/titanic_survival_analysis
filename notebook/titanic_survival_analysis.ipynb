{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e38090b",
   "metadata": {},
   "source": [
    "\n",
    "# Titanic Dataset Preprocessing and Analysis Script\n",
    "================================================\n",
    "\n",
    "This script performs comprehensive preprocessing and analysis of the Titanic dataset,\n",
    "including data cleaning, feature engineering, visualization, and statistical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af940d3",
   "metadata": {},
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 1. IMPORT NECESSARY LIBRARIES\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b47689",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TITANIC DATASET PREPROCESSING AND ANALYSIS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14a9077",
   "metadata": {},
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 2. LOAD THE DATASET\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b27ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset(filepath):\n",
    "    \"\"\"Load the Titanic dataset from CSV file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"‚úì Dataset loaded successfully!\")\n",
    "        print(f\"  Shape: {df.shape}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚úó Error: File '{filepath}' not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the dataset\n",
    "df = load_dataset('test.csv')\n",
    "if df is None:\n",
    "    exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0155c",
   "metadata": {},
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 3. INITIAL INSPECTION\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e50991",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initial_inspection(df):\n",
    "    \"\"\"Perform initial inspection of the dataset.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"INITIAL DATASET INSPECTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Display basic information\n",
    "    print(\"\\nüìä DATASET HEAD:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\nüìã DATASET INFO:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    print(\"\\nüîç MISSING VALUES:\")\n",
    "    missing_vals = df.isnull().sum()\n",
    "    missing_percent = (missing_vals / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing_vals,\n",
    "        'Percentage': missing_percent\n",
    "    }).sort_values('Missing Count', ascending=False)\n",
    "    print(missing_df[missing_df['Missing Count'] > 0])\n",
    "    \n",
    "    print(\"\\nüìà DESCRIPTIVE STATISTICS:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    print(\"\\nüè∑Ô∏è UNIQUE VALUES PER COLUMN:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"{col}: {df[col].nunique()} unique values\")\n",
    "\n",
    "initial_inspection(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a03f6fc",
   "metadata": {},
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 4. CLEAN MISSING VALUES\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10619a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_missing_values(df):\n",
    "    \"\"\"Clean missing values in the dataset.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CLEANING MISSING VALUES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Impute Age with median\n",
    "    if 'Age' in df_clean.columns:\n",
    "        age_median = df_clean['Age'].median()\n",
    "        df_clean['Age'].fillna(age_median, inplace=True)\n",
    "        print(f\"‚úì Age: Filled {df['Age'].isnull().sum()} missing values with median ({age_median:.1f})\")\n",
    "    \n",
    "    # Impute Embarked with mode\n",
    "    if 'Embarked' in df_clean.columns:\n",
    "        embarked_mode = df_clean['Embarked'].mode()[0] if not df_clean['Embarked'].mode().empty else 'S'\n",
    "        df_clean['Embarked'].fillna(embarked_mode, inplace=True)\n",
    "        print(f\"‚úì Embarked: Filled {df['Embarked'].isnull().sum()} missing values with mode ('{embarked_mode}')\")\n",
    "    \n",
    "    # Handle Cabin - convert to binary HasCabin feature\n",
    "    if 'Cabin' in df_clean.columns:\n",
    "        df_clean['HasCabin'] = df_clean['Cabin'].notna().astype(int)\n",
    "        df_clean.drop('Cabin', axis=1, inplace=True)\n",
    "        print(f\"‚úì Cabin: Converted to binary 'HasCabin' feature\")\n",
    "    \n",
    "    # Fill any remaining missing values in Fare with median\n",
    "    if 'Fare' in df_clean.columns and df_clean['Fare'].isnull().any():\n",
    "        fare_median = df_clean['Fare'].median()\n",
    "        df_clean['Fare'].fillna(fare_median, inplace=True)\n",
    "        print(f\"‚úì Fare: Filled {df['Fare'].isnull().sum()} missing values with median ({fare_median:.2f})\")\n",
    "    \n",
    "    print(f\"\\nüìä Missing values after cleaning:\")\n",
    "    remaining_missing = df_clean.isnull().sum().sum()\n",
    "    print(f\"Total missing values: {remaining_missing}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "df_clean = clean_missing_values(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c759a73",
   "metadata": {},
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 5. DROP IRRELEVANT COLUMNS\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4f429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def drop_irrelevant_columns(df):\n",
    "    \"\"\"Drop columns that are not useful for analysis.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DROPPING IRRELEVANT COLUMNS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    columns_to_drop = []\n",
    "    \n",
    "    # Drop PassengerId and Ticket as they don't provide meaningful information\n",
    "    for col in ['PassengerId', 'Ticket']:\n",
    "        if col in df_processed.columns:\n",
    "            columns_to_drop.append(col)\n",
    "    \n",
    "    if columns_to_drop:\n",
    "        df_processed.drop(columns_to_drop, axis=1, inplace=True)\n",
    "        print(f\"‚úì Dropped columns: {columns_to_drop}\")\n",
    "    \n",
    "    print(f\"üìä Remaining columns: {list(df_processed.columns)}\")\n",
    "    return df_processed\n",
    "\n",
    "df_processed = drop_irrelevant_columns(df_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d6add0",
   "metadata": {},
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 6. FEATURE ENGINEERING\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447eb626",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_engineering(df):\n",
    "    \"\"\"Create new features from existing ones.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FEATURE ENGINEERING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df_featured = df.copy()\n",
    "    \n",
    "    # Extract Title from Name\n",
    "    if 'Name' in df_featured.columns:\n",
    "        df_featured['Title'] = df_featured['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "        \n",
    "        # Group rare titles\n",
    "        title_counts = df_featured['Title'].value_counts()\n",
    "        rare_titles = title_counts[title_counts < 10].index\n",
    "        df_featured['Title'] = df_featured['Title'].replace(rare_titles, 'Rare')\n",
    "        \n",
    "        print(f\"‚úì Title: Extracted from Name column\")\n",
    "        print(f\"  Title distribution: {df_featured['Title'].value_counts().to_dict()}\")\n",
    "        \n",
    "        # Now we can drop the Name column\n",
    "        df_featured.drop('Name', axis=1, inplace=True)\n",
    "        print(f\"‚úì Name: Dropped after title extraction\")\n",
    "    \n",
    "    # Create FamilySize\n",
    "    if 'SibSp' in df_featured.columns and 'Parch' in df_featured.columns:\n",
    "        df_featured['FamilySize'] = df_featured['SibSp'] + df_featured['Parch'] + 1\n",
    "        print(f\"‚úì FamilySize: Created (SibSp + Parch + 1)\")\n",
    "        print(f\"  FamilySize range: {df_featured['FamilySize'].min()} to {df_featured['FamilySize'].max()}\")\n",
    "    \n",
    "    # Create IsAlone\n",
    "    if 'FamilySize' in df_featured.columns:\n",
    "        df_featured['IsAlone'] = (df_featured['FamilySize'] == 1).astype(int)\n",
    "        alone_count = df_featured['IsAlone'].sum()\n",
    "        print(f\"‚úì IsAlone: Created ({alone_count} passengers traveling alone)\")\n",
    "    \n",
    "    return df_featured\n",
    "\n",
    "df_featured = feature_engineering(df_processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144c0137",
   "metadata": {},
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 7. ENCODE CATEGORICAL VARIABLES\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fa3a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_categorical_variables(df):\n",
    "    \"\"\"Encode categorical variables for analysis.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ENCODING CATEGORICAL VARIABLES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # Binary encoding for Sex\n",
    "    if 'Sex' in df_encoded.columns:\n",
    "        df_encoded['Sex'] = df_encoded['Sex'].map({'male': 1, 'female': 0})\n",
    "        print(\"‚úì Sex: Encoded (male=1, female=0)\")\n",
    "    \n",
    "    # Label encoding for other categorical variables\n",
    "    categorical_cols = ['Embarked', 'Title']\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df_encoded.columns:\n",
    "            le = LabelEncoder()\n",
    "            df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "            print(f\"‚úì {col}: Label encoded\")\n",
    "            print(f\"  Mapping: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "    \n",
    "    # Pclass is already numeric, but let's ensure it's properly formatted\n",
    "    if 'Pclass' in df_encoded.columns:\n",
    "        df_encoded['Pclass'] = df_encoded['Pclass'].astype(int)\n",
    "        print(\"‚úì Pclass: Ensured integer format\")\n",
    "    \n",
    "    return df_encoded, label_encoders\n",
    "\n",
    "df_encoded, encoders = encode_categorical_variables(df_featured)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851938c6",
   "metadata": {},
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 8. CHECK FOR DUPLICATES\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75035621",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_duplicates(df):\n",
    "    \"\"\"Check for and remove duplicate rows.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CHECKING FOR DUPLICATES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    initial_shape = df.shape\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    \n",
    "    print(f\"üìä Duplicate rows found: {duplicate_count}\")\n",
    "    \n",
    "    if duplicate_count > 0:\n",
    "        df_no_dupes = df.drop_duplicates()\n",
    "        print(f\"‚úì Removed {duplicate_count} duplicate rows\")\n",
    "        print(f\"  Shape before: {initial_shape}\")\n",
    "        print(f\"  Shape after: {df_no_dupes.shape}\")\n",
    "        return df_no_dupes\n",
    "    else:\n",
    "        print(\"‚úì No duplicate rows found\")\n",
    "        return df\n",
    "\n",
    "df_no_dupes = check_duplicates(df_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bc652e",
   "metadata": {},
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 9. HANDLE OUTLIERS\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d9700",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def handle_outliers(df):\n",
    "    \"\"\"Identify and handle outliers in numerical columns.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"HANDLING OUTLIERS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df_outliers = df.copy()\n",
    "    \n",
    "    # Create boxplots for Fare and Age\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Fare boxplot\n",
    "    if 'Fare' in df_outliers.columns:\n",
    "        axes[0].boxplot(df_outliers['Fare'].dropna())\n",
    "        axes[0].set_title('Fare Distribution (Before Transformation)')\n",
    "        axes[0].set_ylabel('Fare')\n",
    "        \n",
    "        # Check if log transformation is needed\n",
    "        fare_skewness = df_outliers['Fare'].skew()\n",
    "        print(f\"üìä Fare skewness: {fare_skewness:.3f}\")\n",
    "        \n",
    "        if fare_skewness > 1:  # Highly skewed\n",
    "            # Add small constant to handle zero values\n",
    "            df_outliers['Fare_log'] = np.log1p(df_outliers['Fare'])\n",
    "            print(\"‚úì Fare: Applied log transformation due to high skewness\")\n",
    "            print(f\"  New skewness: {df_outliers['Fare_log'].skew():.3f}\")\n",
    "    \n",
    "    # Age boxplot\n",
    "    if 'Age' in df_outliers.columns:\n",
    "        axes[1].boxplot(df_outliers['Age'].dropna())\n",
    "        axes[1].set_title('Age Distribution')\n",
    "        axes[1].set_ylabel('Age')\n",
    "        \n",
    "        age_skewness = df_outliers['Age'].skew()\n",
    "        print(f\"üìä Age skewness: {age_skewness:.3f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outlier_boxplots.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"‚úì Boxplots saved as 'outlier_boxplots.png'\")\n",
    "    \n",
    "    return df_outliers\n",
    "\n",
    "df_outliers = handle_outliers(df_no_dupes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7d8d87",
   "metadata": {},
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 10. NORMALIZE/STANDARDIZE NUMERICAL FEATURES\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe00c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_features(df):\n",
    "    \"\"\"Normalize numerical features using StandardScaler.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"NORMALIZING NUMERICAL FEATURES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df_normalized = df.copy()\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Features to normalize\n",
    "    features_to_scale = []\n",
    "    \n",
    "    if 'Age' in df_normalized.columns:\n",
    "        features_to_scale.append('Age')\n",
    "    \n",
    "    # Use log-transformed Fare if available, otherwise original Fare\n",
    "    if 'Fare_log' in df_normalized.columns:\n",
    "        features_to_scale.append('Fare_log')\n",
    "    elif 'Fare' in df_normalized.columns:\n",
    "        features_to_scale.append('Fare')\n",
    "    \n",
    "    if features_to_scale:\n",
    "        df_normalized[features_to_scale] = scaler.fit_transform(df_normalized[features_to_scale])\n",
    "        print(f\"‚úì Normalized features: {features_to_scale}\")\n",
    "        \n",
    "        for feature in features_to_scale:\n",
    "            mean_val = df_normalized[feature].mean()\n",
    "            std_val = df_normalized[feature].std()\n",
    "            print(f\"  {feature}: mean={mean_val:.3f}, std={std_val:.3f}\")\n",
    "    \n",
    "    return df_normalized, scaler\n",
    "\n",
    "df_final, scaler = normalize_features(df_outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea44b4a0",
   "metadata": {},
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 11. VISUALIZATIONS\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e6f387",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_visualizations(df):\n",
    "    \"\"\"Create comprehensive visualizations of the dataset.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CREATING VISUALIZATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Set up the plotting environment\n",
    "    plt.rcParams['figure.figsize'] = (12, 8)\n",
    "    \n",
    "    # 1. Correlation Heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Select only numeric columns for correlation\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "    \n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', \n",
    "                center=0, square=True, fmt='.2f')\n",
    "    plt.title('Feature Correlation Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"‚úì Correlation heatmap saved as 'correlation_heatmap.png'\")\n",
    "    \n",
    "    # 2. Distribution plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Age distribution\n",
    "    if 'Age' in df.columns:\n",
    "        axes[0, 0].hist(df['Age'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[0, 0].set_title('Age Distribution')\n",
    "        axes[0, 0].set_xlabel('Age')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Fare distribution\n",
    "    fare_col = 'Fare_log' if 'Fare_log' in df.columns else 'Fare'\n",
    "    if fare_col in df.columns:\n",
    "        axes[0, 1].hist(df[fare_col], bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "        title = 'Fare Distribution (Log-transformed)' if fare_col == 'Fare_log' else 'Fare Distribution'\n",
    "        axes[0, 1].set_title(title)\n",
    "        axes[0, 1].set_xlabel(fare_col)\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Sex distribution\n",
    "    if 'Sex' in df.columns:\n",
    "        sex_counts = df['Sex'].value_counts()\n",
    "        sex_labels = ['Female' if x == 0 else 'Male' for x in sex_counts.index]\n",
    "        axes[1, 0].bar(sex_labels, sex_counts.values, color=['pink', 'lightblue'])\n",
    "        axes[1, 0].set_title('Gender Distribution')\n",
    "        axes[1, 0].set_ylabel('Count')\n",
    "    \n",
    "    # Pclass distribution\n",
    "    if 'Pclass' in df.columns:\n",
    "        pclass_counts = df['Pclass'].value_counts().sort_index()\n",
    "        axes[1, 1].bar(pclass_counts.index, pclass_counts.values, color='orange', alpha=0.7)\n",
    "        axes[1, 1].set_title('Passenger Class Distribution')\n",
    "        axes[1, 1].set_xlabel('Class')\n",
    "        axes[1, 1].set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('distribution_plots.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"‚úì Distribution plots saved as 'distribution_plots.png'\")\n",
    "    \n",
    "    # 3. Count plots for categorical variables\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Sex vs other variables (using original data for better interpretation)\n",
    "    original_df = pd.read_csv('test.csv')  # Reload for better labels\n",
    "    \n",
    "    if 'Sex' in original_df.columns and 'Pclass' in original_df.columns:\n",
    "        sns.countplot(data=original_df, x='Sex', hue='Pclass', ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Gender by Passenger Class')\n",
    "        axes[0, 0].legend(title='Class')\n",
    "    \n",
    "    if 'Sex' in original_df.columns and 'Embarked' in original_df.columns:\n",
    "        sns.countplot(data=original_df, x='Sex', hue='Embarked', ax=axes[0, 1])\n",
    "        axes[0, 1].set_title('Gender by Embarkation Port')\n",
    "        axes[0, 1].legend(title='Embarked')\n",
    "    \n",
    "    if 'Pclass' in original_df.columns and 'Embarked' in original_df.columns:\n",
    "        sns.countplot(data=original_df, x='Pclass', hue='Embarked', ax=axes[1, 0])\n",
    "        axes[1, 0].set_title('Passenger Class by Embarkation Port')\n",
    "        axes[1, 0].legend(title='Embarked')\n",
    "    \n",
    "    # Family Size distribution\n",
    "    if 'FamilySize' in df.columns:\n",
    "        family_counts = df['FamilySize'].value_counts().sort_index()\n",
    "        axes[1, 1].bar(family_counts.index, family_counts.values, color='purple', alpha=0.7)\n",
    "        axes[1, 1].set_title('Family Size Distribution')\n",
    "        axes[1, 1].set_xlabel('Family Size')\n",
    "        axes[1, 1].set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('categorical_plots.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"‚úì Categorical plots saved as 'categorical_plots.png'\")\n",
    "    \n",
    "    print(\"‚úì All visualizations completed successfully!\")\n",
    "\n",
    "create_visualizations(df_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37132990",
   "metadata": {},
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 12. SUMMARY STATISTICS\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef7f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_summary_statistics(df, original_df):\n",
    "    \"\"\"Generate comprehensive summary statistics.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic dataset statistics\n",
    "    print(\"üìä DATASET OVERVIEW:\")\n",
    "    print(f\"  Total records: {len(df)}\")\n",
    "    print(f\"  Total features: {len(df.columns)}\")\n",
    "    print(f\"  Features: {list(df.columns)}\")\n",
    "    \n",
    "    # Age statistics\n",
    "    if 'Age' in original_df.columns:\n",
    "        print(f\"\\nüìä AGE STATISTICS:\")\n",
    "        print(f\"  Mean age: {original_df['Age'].mean():.1f} years\")\n",
    "        print(f\"  Median age: {original_df['Age'].median():.1f} years\")\n",
    "        print(f\"  Age range: {original_df['Age'].min():.1f} - {original_df['Age'].max():.1f} years\")\n",
    "    \n",
    "    # Fare statistics\n",
    "    if 'Fare' in original_df.columns:\n",
    "        print(f\"\\nüí∞ FARE STATISTICS:\")\n",
    "        print(f\"  Mean fare: ${original_df['Fare'].mean():.2f}\")\n",
    "        print(f\"  Median fare: ${original_df['Fare'].median():.2f}\")\n",
    "        print(f\"  Fare range: ${original_df['Fare'].min():.2f} - ${original_df['Fare'].max():.2f}\")\n",
    "    \n",
    "    # Class distribution\n",
    "    if 'Pclass' in original_df.columns:\n",
    "        print(f\"\\nüé´ PASSENGER CLASS DISTRIBUTION:\")\n",
    "        class_dist = original_df['Pclass'].value_counts().sort_index()\n",
    "        for class_num, count in class_dist.items():\n",
    "            percentage = (count / len(original_df)) * 100\n",
    "            print(f\"  Class {class_num}: {count} passengers ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Gender distribution\n",
    "    if 'Sex' in original_df.columns:\n",
    "        print(f\"\\nüë• GENDER DISTRIBUTION:\")\n",
    "        gender_dist = original_df['Sex'].value_counts()\n",
    "        for gender, count in gender_dist.items():\n",
    "            percentage = (count / len(original_df)) * 100\n",
    "            print(f\"  {gender.title()}: {count} passengers ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Embarkation statistics\n",
    "    if 'Embarked' in original_df.columns:\n",
    "        print(f\"\\n‚öì EMBARKATION PORT DISTRIBUTION:\")\n",
    "        embarked_dist = original_df['Embarked'].value_counts()\n",
    "        port_names = {'S': 'Southampton', 'C': 'Cherbourg', 'Q': 'Queenstown'}\n",
    "        for port, count in embarked_dist.items():\n",
    "            percentage = (count / len(original_df)) * 100\n",
    "            port_name = port_names.get(port, port)\n",
    "            print(f\"  {port_name} ({port}): {count} passengers ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Family statistics\n",
    "    if 'FamilySize' in df.columns:\n",
    "        print(f\"\\nüë®‚Äçüë©‚Äçüëß‚Äçüë¶ FAMILY STATISTICS:\")\n",
    "        print(f\"  Average family size: {df['FamilySize'].mean():.1f}\")\n",
    "        print(f\"  Passengers traveling alone: {df['IsAlone'].sum()} ({(df['IsAlone'].sum()/len(df)*100):.1f}%)\")\n",
    "        \n",
    "        family_dist = df['FamilySize'].value_counts().sort_index()\n",
    "        print(f\"  Family size distribution:\")\n",
    "        for size, count in family_dist.items():\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"    Size {size}: {count} passengers ({percentage:.1f}%)\")\n",
    "\n",
    "# Load original data for better summary statistics\n",
    "original_df = pd.read_csv('test.csv')\n",
    "generate_summary_statistics(df_final, original_df)\n",
    "\n",
    "# ============================================================================\n",
    "# 13. EXPORT CLEANED DATASET\n",
    "# ============================================================================\n",
    "\n",
    "def export_cleaned_dataset(df, filename='titanic_cleaned.csv'):\n",
    "    \"\"\"Export the cleaned and processed dataset.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EXPORTING CLEANED DATASET\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"‚úì Cleaned dataset exported successfully!\")\n",
    "        print(f\"  Filename: {filename}\")\n",
    "        print(f\"  Shape: {df.shape}\")\n",
    "        print(f\"  Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Display first few rows of the cleaned dataset\n",
    "        print(f\"\\nüìä CLEANED DATASET PREVIEW:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Show data types\n",
    "        print(f\"\\nüìã FINAL DATA TYPES:\")\n",
    "        for col, dtype in df.dtypes.items():\n",
    "            print(f\"  {col}: {dtype}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error exporting dataset: {e}\")\n",
    "\n",
    "export_cleaned_dataset(df_final)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b933b68",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95062e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PREPROCESSING COMPLETED SUCCESSFULLY! üéâ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìã PROCESSING SUMMARY:\")\n",
    "print(\"‚úì Dataset loaded and inspected\")\n",
    "print(\"‚úì Missing values handled\")\n",
    "print(\"‚úì Irrelevant columns removed\")\n",
    "print(\"‚úì New features engineered\")\n",
    "print(\"‚úì Categorical variables encoded\")\n",
    "print(\"‚úì Duplicates checked and removed\")\n",
    "print(\"‚úì Outliers identified and handled\")\n",
    "print(\"‚úì Numerical features normalized\")\n",
    "print(\"‚úì Comprehensive visualizations created\")\n",
    "print(\"‚úì Summary statistics generated\")\n",
    "print(\"‚úì Cleaned dataset exported\")\n",
    "\n",
    "print(\"\\nüìÅ FILES CREATED:\")\n",
    "print(\"‚Ä¢ titanic_cleaned.csv - Cleaned and processed dataset\")\n",
    "print(\"‚Ä¢ correlation_heatmap.png - Feature correlation visualization\")\n",
    "print(\"‚Ä¢ distribution_plots.png - Feature distribution plots\")\n",
    "print(\"‚Ä¢ categorical_plots.png - Categorical variable visualizations\")\n",
    "print(\"‚Ä¢ outlier_boxplots.png - Outlier detection plots\")\n",
    "\n",
    "print(f\"\\nüéØ FINAL DATASET SHAPE: {df_final.shape}\")\n",
    "print(f\"üìä FEATURES READY FOR ANALYSIS: {len(df_final.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"The dataset is now ready for machine learning modeling!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
