{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Dataset Preprocessing Pipeline\n",
    "\n",
    "This notebook provides a comprehensive preprocessing pipeline for the Titanic dataset, including:\n",
    "- Data loading and inspection\n",
    "- Missing value handling\n",
    "- Feature engineering\n",
    "- Categorical encoding\n",
    "- Outlier detection and handling\n",
    "- Data normalization\n",
    "- Comprehensive visualizations\n",
    "- Summary statistics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_titanic_data(filepath='data/train.csv'):\n",
    "    \"\"\"\n",
    "    Load the Titanic dataset from CSV file.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to the CSV file\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"‚úì Dataset loaded successfully from {filepath}\")\n",
    "        print(f\"  Shape: {df.shape}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚úó Error: File '{filepath}' not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the dataset\n",
    "print(\"=\" * 60)\n",
    "print(\"TITANIC DATASET LOADING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df = load_titanic_data('data/train.csv')\n",
    "if df is None:\n",
    "    print(\"‚ùå Failed to load dataset. Please check the file path.\")\n",
    "else:\n",
    "    print(f\"üéâ Dataset loaded successfully with {df.shape[0]} rows and {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_dataset(df):\n",
    "    \"\"\"\n",
    "    Perform initial inspection of the dataset.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset to inspect\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"DATASET INSPECTION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\nüìä Dataset Head:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    print(\"\\nüìã Dataset Info:\")\n",
    "    df.info()\n",
    "    \n",
    "    print(\"\\nüîç Missing Values:\")\n",
    "    missing_vals = df.isnull().sum()\n",
    "    missing_percent = (missing_vals / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing_vals,\n",
    "        'Percentage': missing_percent\n",
    "    }).sort_values('Missing Count', ascending=False)\n",
    "    display(missing_df[missing_df['Missing Count'] > 0])\n",
    "    \n",
    "    print(\"\\nüìà Descriptive Statistics:\")\n",
    "    display(df.describe())\n",
    "    \n",
    "    print(\"\\nüè∑Ô∏è Unique Values:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"  {col}: {df[col].nunique()} unique values\")\n",
    "\n",
    "# Inspect the dataset\n",
    "if df is not None:\n",
    "    inspect_dataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Missing Value Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_missing_values(df):\n",
    "    \"\"\"\n",
    "    Clean missing values in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset to clean\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned dataset\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"CLEANING MISSING VALUES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Impute Age with median\n",
    "    if 'Age' in df_clean.columns and df_clean['Age'].isnull().any():\n",
    "        age_median = df_clean['Age'].median()\n",
    "        missing_age = df['Age'].isnull().sum()\n",
    "        df_clean['Age'].fillna(age_median, inplace=True)\n",
    "        print(f\"‚úì Age: Filled {missing_age} missing values with median ({age_median:.1f})\")\n",
    "    \n",
    "    # Impute Embarked with mode\n",
    "    if 'Embarked' in df_clean.columns and df_clean['Embarked'].isnull().any():\n",
    "        embarked_mode = df_clean['Embarked'].mode()[0] if not df_clean['Embarked'].mode().empty else 'S'\n",
    "        missing_embarked = df['Embarked'].isnull().sum()\n",
    "        df_clean['Embarked'].fillna(embarked_mode, inplace=True)\n",
    "        print(f\"‚úì Embarked: Filled {missing_embarked} missing values with mode ('{embarked_mode}')\")\n",
    "    \n",
    "    # Handle Cabin - convert to binary HasCabin feature\n",
    "    if 'Cabin' in df_clean.columns:\n",
    "        df_clean['HasCabin'] = df_clean['Cabin'].notna().astype(int)\n",
    "        df_clean.drop('Cabin', axis=1, inplace=True)\n",
    "        print(\"‚úì Cabin: Converted to binary 'HasCabin' feature\")\n",
    "    \n",
    "    # Fill Fare with median if missing\n",
    "    if 'Fare' in df_clean.columns and df_clean['Fare'].isnull().any():\n",
    "        fare_median = df_clean['Fare'].median()\n",
    "        missing_fare = df['Fare'].isnull().sum()\n",
    "        df_clean['Fare'].fillna(fare_median, inplace=True)\n",
    "        print(f\"‚úì Fare: Filled {missing_fare} missing values with median ({fare_median:.2f})\")\n",
    "    \n",
    "    remaining_missing = df_clean.isnull().sum().sum()\n",
    "    print(f\"\\nüìä Total missing values after cleaning: {remaining_missing}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Clean missing values\n",
    "if df is not None:\n",
    "    df_clean = clean_missing_values(df)\n",
    "    print(f\"\\nüéØ Dataset shape after cleaning: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Remove Irrelevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_irrelevant_columns(df):\n",
    "    \"\"\"\n",
    "    Drop columns that are not useful for analysis.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset to process\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset with irrelevant columns removed\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"DROPPING IRRELEVANT COLUMNS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    columns_to_drop = []\n",
    "    \n",
    "    # Drop PassengerId and Ticket\n",
    "    for col in ['PassengerId', 'Ticket']:\n",
    "        if col in df_processed.columns:\n",
    "            columns_to_drop.append(col)\n",
    "    \n",
    "    if columns_to_drop:\n",
    "        df_processed.drop(columns_to_drop, axis=1, inplace=True)\n",
    "        print(f\"‚úì Dropped columns: {columns_to_drop}\")\n",
    "    \n",
    "    print(f\"üìä Remaining columns: {list(df_processed.columns)}\")\n",
    "    return df_processed\n",
    "\n",
    "# Drop irrelevant columns\n",
    "if 'df_clean' in locals():\n",
    "    df_processed = drop_irrelevant_columns(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Check for Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(df):\n",
    "    \"\"\"\n",
    "    Check for and remove duplicate rows.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset to check\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset with duplicates removed\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"CHECKING DUPLICATES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    print(f\"üìä Duplicate rows found: {duplicate_count}\")\n",
    "    \n",
    "    if duplicate_count > 0:\n",
    "        df_no_dupes = df.drop_duplicates()\n",
    "        print(f\"‚úì Removed {duplicate_count} duplicate rows\")\n",
    "        return df_no_dupes\n",
    "    else:\n",
    "        print(\"‚úì No duplicate rows found\")\n",
    "        return df\n",
    "\n",
    "# Check for duplicates\n",
    "if 'df_processed' in locals():\n",
    "    df_no_dupes = check_duplicates(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_features(df):\n",
    "    \"\"\"\n",
    "    Create new features from existing ones.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset to enhance\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset with new features\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"FEATURE ENGINEERING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    df_featured = df.copy()\n",
    "    \n",
    "    # Extract Title from Name\n",
    "    if 'Name' in df_featured.columns:\n",
    "        df_featured['Title'] = df_featured['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "        \n",
    "        # Group rare titles\n",
    "        title_counts = df_featured['Title'].value_counts()\n",
    "        rare_titles = title_counts[title_counts < 10].index\n",
    "        df_featured['Title'] = df_featured['Title'].replace(rare_titles, 'Rare')\n",
    "        \n",
    "        print(\"‚úì Title: Extracted from Name column\")\n",
    "        print(f\"  Distribution: {df_featured['Title'].value_counts().to_dict()}\")\n",
    "        \n",
    "        # Drop Name column after title extraction\n",
    "        df_featured.drop('Name', axis=1, inplace=True)\n",
    "        print(\"‚úì Name: Dropped after title extraction\")\n",
    "    \n",
    "    # Create FamilySize\n",
    "    if 'SibSp' in df_featured.columns and 'Parch' in df_featured.columns:\n",
    "        df_featured['FamilySize'] = df_featured['SibSp'] + df_featured['Parch'] + 1\n",
    "        print(f\"‚úì FamilySize: Created (range: {df_featured['FamilySize'].min()}-{df_featured['FamilySize'].max()})\")\n",
    "    \n",
    "    # Create IsAlone\n",
    "    if 'FamilySize' in df_featured.columns:\n",
    "        df_featured['IsAlone'] = (df_featured['FamilySize'] == 1).astype(int)\n",
    "        alone_count = df_featured['IsAlone'].sum()\n",
    "        print(f\"‚úì IsAlone: Created ({alone_count} passengers traveling alone)\")\n",
    "    \n",
    "    return df_featured\n",
    "\n",
    "# Create new features\n",
    "if 'df_no_dupes' in locals():\n",
    "    df_featured = create_new_features(df_no_dupes)\n",
    "    print(f\"\\nüéØ Dataset shape after feature engineering: {df_featured.shape}\")\n",
    "    print(f\"üìä New columns: {list(df_featured.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Categorical Variable Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_variables(df):\n",
    "    \"\"\"\n",
    "    Encode categorical variables for analysis.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset to encode\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (encoded_dataset, label_encoders_dict)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ENCODING CATEGORICAL VARIABLES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    df_encoded = df.copy()\n",
    "    label_encoders = {}\n",
    "    \n",
    "    # Binary encoding for Sex\n",
    "    if 'Sex' in df_encoded.columns:\n",
    "        df_encoded['Sex'] = df_encoded['Sex'].map({'male': 1, 'female': 0})\n",
    "        print(\"‚úì Sex: Encoded (male=1, female=0)\")\n",
    "    \n",
    "    # Label encoding for other categorical variables\n",
    "    categorical_cols = ['Embarked', 'Title']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df_encoded.columns:\n",
    "            le = LabelEncoder()\n",
    "            df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "            mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "            print(f\"‚úì {col}: Label encoded - {mapping}\")\n",
    "    \n",
    "    # Ensure Pclass is integer\n",
    "    if 'Pclass' in df_encoded.columns:\n",
    "        df_encoded['Pclass'] = df_encoded['Pclass'].astype(int)\n",
    "        print(\"‚úì Pclass: Ensured integer format\")\n",
    "    \n",
    "    return df_encoded, label_encoders\n",
    "\n",
    "# Encode categorical variables\n",
    "if 'df_featured' in locals():\n",
    "    df_encoded, encoders = encode_categorical_variables(df_featured)\n",
    "    print(f\"\\nüéØ Dataset shape after encoding: {df_encoded.shape}\")\n",
    "    print(f\"üìã Encoders saved: {list(encoders.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Outlier Detection and Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers(df):\n",
    "    \"\"\"\n",
    "    Identify and handle outliers in numerical columns.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset to process\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset with outliers handled\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"HANDLING OUTLIERS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    df_outliers = df.copy()\n",
    "    \n",
    "    # Create boxplots for Fare and Age\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Fare analysis and transformation\n",
    "    if 'Fare' in df_outliers.columns:\n",
    "        axes[0].boxplot(df_outliers['Fare'].dropna())\n",
    "        axes[0].set_title('Fare Distribution (Before)')\n",
    "        axes[0].set_ylabel('Fare')\n",
    "        \n",
    "        fare_skewness = df_outliers['Fare'].skew()\n",
    "        print(f\"üìä Fare skewness: {fare_skewness:.3f}\")\n",
    "        \n",
    "        if fare_skewness > 1:  # Highly skewed\n",
    "            df_outliers['Fare_log'] = np.log1p(df_outliers['Fare'])\n",
    "            print(f\"‚úì Fare: Applied log transformation (new skewness: {df_outliers['Fare_log'].skew():.3f})\")\n",
    "    \n",
    "    # Age analysis\n",
    "    if 'Age' in df_outliers.columns:\n",
    "        axes[1].boxplot(df_outliers['Age'].dropna())\n",
    "        axes[1].set_title('Age Distribution')\n",
    "        axes[1].set_ylabel('Age')\n",
    "        \n",
    "        age_skewness = df_outliers['Age'].skew()\n",
    "        print(f\"üìä Age skewness: {age_skewness:.3f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"‚úì Outlier analysis plots created\")\n",
    "    \n",
    "    return df_outliers\n",
    "\n",
    "# Handle outliers\n",
    "if 'df_encoded' in locals():\n",
    "    df_outliers = handle_outliers(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(df):\n",
    "    \"\"\"\n",
    "    Normalize numerical features using StandardScaler.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset to normalize\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (normalized_dataset, scaler_object)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"NORMALIZING FEATURES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    df_normalized = df.copy()\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Features to normalize\n",
    "    features_to_scale = []\n",
    "    \n",
    "    if 'Age' in df_normalized.columns:\n",
    "        features_to_scale.append('Age')\n",
    "    \n",
    "    # Use log-transformed Fare if available, otherwise original Fare\n",
    "    if 'Fare_log' in df_normalized.columns:\n",
    "        features_to_scale.append('Fare_log')\n",
    "    elif 'Fare' in df_normalized.columns:\n",
    "        features_to_scale.append('Fare')\n",
    "    \n",
    "    if features_to_scale:\n",
    "        df_normalized[features_to_scale] = scaler.fit_transform(df_normalized[features_to_scale])\n",
    "        print(f\"‚úì Normalized features: {features_to_scale}\")\n",
    "        \n",
    "        for feature in features_to_scale:\n",
    "            mean_val = df_normalized[feature].mean()\n",
    "            std_val = df_normalized[feature].std()\n",
    "            print(f\"  {feature}: mean={mean_val:.3f}, std={std_val:.3f}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No numerical features found for normalization\")\n",
    "    \n",
    "    return df_normalized, scaler\n",
    "\n",
    "# Normalize features\n",
    "if 'df_outliers' in locals():\n",
    "    df_normalized, scaler = normalize_features(df_outliers)\n",
    "    print(f\"\\nüéØ Final dataset shape: {df_normalized.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap\n",
    "if 'df_normalized' in locals():\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    numeric_cols = df_normalized.select_dtypes(include=[np.number]).columns\n",
    "    correlation_matrix = df_normalized[numeric_cols].corr()\n",
    "    \n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm',\n",
    "                center=0, square=True, fmt='.2f')\n",
    "    plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"‚úì Correlation heatmap created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Distribution Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots\n",
    "if 'df_normalized' in locals():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Age distribution\n",
    "    if 'Age' in df_normalized.columns:\n",
    "        axes[0, 0].hist(df_normalized['Age'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[0, 0].set_title('Age Distribution (Normalized)', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Age (Normalized)')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Fare distribution\n",
    "    fare_col = 'Fare_log' if 'Fare_log' in df_normalized.columns else 'Fare'\n",
    "    if fare_col in df_normalized.columns:\n",
    "        axes[0, 1].hist(df_normalized[fare_col], bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "        title = 'Fare Distribution (Log-transformed & Normalized)' if fare_col == 'Fare_log' else 'Fare Distribution (Normalized)'\n",
    "        axes[0, 1].set_title(title, fontweight='bold')\n",
    "        axes[0, 1].set_xlabel(f'{fare_col} (Normalized)')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
